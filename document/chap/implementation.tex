\chapter{Implementation}

This chapter provides an in-depth exploration of the implementation of the S3 proxy, which incorporates caching functionality. The resulting software was given the simple name \textit{s3p} (s3 proxy) and will from here on out be referred to as such. This chapter aims to present a comprehensive overview of the design decisions, technologies employed, and the actual development process. This chapter will further explain the key features and limitations of the implementation.
\\\\
To ensure a robust and efficient implementation, the implementation was done in the Rust programming language. Rust's focus on memory safety, performance, and concurrency aligns well with our project requirements. Leveraging Rust's powerful abstractions, expressive syntax, and rich ecosystem was an important factor in building a reliable and high-performance S3 proxy that integrates advanced caching functionality.

\section{Decisions}

This section deals with the decisions made as part of the design and implementation of the software. For that purpose, it explains the necessity and reasoning behind these decisions, as well as the benefits and limitations they cause.

\subsection{Modularity}

As part of the \textit{internal composition}(\ref{def:composition}) goal, much effort was put into designing components in a way that allows them to be decoupled from each other by abstracting their functionality through simple \code{traits}. As such, the specific implementations of components explained in this chapter should be viewed as building blocks that can be combined in the shape described by the architecture(\ref{architecture}).
This allows the code to be easily expanded to provide support for systems other than S3 in the future, simply by providing additional implementations to the individual components.

\subsection{Configurability}

\subsection{Security}
\label{security}

Since requests to S3 are usually authenticated through a signature unique to each request there needed to be considerations on if and how these requests can be safely cached without exposing data to unauthorized access.

There were plans to implement bucket policy validations inside the proxy. This would enable failing early in the case that a request was sent with insufficient permission, reducing bandwidth and server load. However, there are some major issues with this idea.
Most significantly, a full permissions check would require access to the credentials of all users. Exposing all credentials to the proxy would pose a significant risk and increase the attack surface. As there is no API access to the credentials defined in the S3 specification, the implementation would also not be universally applicable to all S3-compatible services.
A solution would be to only make the proxy accessible through a set of known credentials against which it could validate the bucket policies. After careful evaluation, this solution was ultimately judged to be out of scope for this work, as in this case requesters can be assumed to have knowledge of the proxy and therefore can be expected to have the correct credentials, in which case there would be limited room for acceleration.

The solution that was eventually adopted is that the proxy can optionally be configured to validate incoming requests against a provided set of credentials (A), but will replace the signature with one calculated from its own credentials (B) before forwarding the request.
Although acceptable within the scope of this work, this approach does come with some issues and poses some limitations that should be considered. 
\begin{enumerate}
	\item If A is configured and B has permissions that A does not have, A can bypass this lack of permissions. Conversely, if A has permissions that B does not have, downstream requests may fail unexpectedly. This can be mitigated by using the same set of credentials for A and B.
	\item If A is configured, a request's signature must match A even if the bucket policy would allow for public access.
	\item If no credentials are configured for A, all requests to the proxy will be executed with B without any validation of signatures on the incoming request. Exposing the proxy in this state poses a significant risk of unintended unauthorized access.
\end{enumerate}

\section{Design Process}

After designing the architecture (\ref{architecture}), the first step to implementation was to outline the traits necessary to represent the \code{Server}, \code{Middleware} and \code{Client} components as well as design a crude API for each component. Then, the flow of a request through the system was implemented as part of the \code{Pipeline}. Initially, the plan was to pass an MPSC queue to the server that received each request from the server component and was then forwarded to the middleware stack and finally the client by the \code{Pipeline}. This design proved to be impractical due to multiple reasons:
\begin{enumerate}
	\item Multiple requests can be accepted by the server concurrently. Therefore responses to the request may be generated out of order. By releasing the requests on a message queue, the association between the task accepting the request and its completion was lost. To resolve this, additional fields had to be added to the \code{Request} type, inhibiting its use as an abstraction.
	\item Rust's concurrency model is largely built around futures. The handling of requests released on the queue could only be associated with a future by adding a completion handler to the Request type analogous to 1.
	\item Messaging queues performed significantly worse than passing callables to the server.
	\item Conflicts with Rust's ownership system
\end{enumerate}


\section{Configuration}
\label{configuration}

Since configurability was part of the stated goals (\ref{goals}), the intended configuration schema was loosely defined very early on in the implementation. It uses a single TOML file, through which all configurations can be accessed. The configuration file is passed to the program on startup via CLI arguments and through it the internal composition is defined. During startup, the file is parsed and the individual components are dynamically configured at runtime. Within this chapter, each component will introduce its own specific configuration.

\section{The Server component}

The server is the component responsible for accepting requests made to the proxy. It is represented by the \code{Server} trait. 
It is initialized by calling the \code{serve} method of a \code{ServerBuilder} with a \code{Handler} to which it will forward all accepted requests. The \code{Handler} trait is implemented for any callable that receives a \code{Request} and returns a future that will resolve to a \code{Response} or an error type.

\begin{multicols}{2}
	\begin{codeblock}{ServerBuilder}{Rust}
		\begin{rustcode}
			pub trait ServerBuilder {
				fn broadcast(&mut self, tx: &BroadcastSend) -> &mut Self;
				fn serve(&self, handler: impl Handler) -> Result<impl Server, Report>;
			}
		\end{rustcode}
	\end{codeblock}

	\begin{codeblock}{Server}{Rust}
		\begin{rustcode}
			pub trait Server: Send {
				async fn stop(self) -> Result<(), Report>;
			}
		\end{rustcode}
	\end{codeblock}
\end{multicols}

The \code{S3Server} is the S3-specific implementation of the \code{Server} trait. It is built upon a traditional HTTP server provided by \code{hyper} and supports requests over HTTP/1.1 and HTTP/2.0. Upon receiving a request, it first validates the request's signature against a set of credentials that can be set inside the configuration file. If the validation was successful, it then determines the S3 operation associated with the request. This functionality required the modifications of the \code{s3s} library described towards the end of this chapter.

\begin{codeblock}{Example server configuration}{TOML}
	\begin{javacode}
		[server]
		type = "s3"
		host = "127.0.0.1"
		port = 4356
		validateCredentials = false
	\end{javacode}
\end{codeblock}

\section{Caching}

The caching logic was designed to improve the overall performance and efficiency of S3 operations. For this purpose, the caching logic was highly customized and strongly differs from traditional HTTP cache implementations. By leveraging knowledge of S3 operations it offers features that exceed the capabilities provided by HTTP caching headers (\ref{http_cache_policy}), but also attempts to be compliant to caching as defined by the HTTP standards where possible. This section explains the design of the caching functionality and explores the advantages and limitations compared to traditional caching systems.

\subsection{Implementation overview}

Caching is implemented by the \code{CacheLayer} middleware and is designed to allow for different cache logic depending on the S3 operation performed by a request. The results of operations were determined to be cacheable if they do not mutate the data or metadata of objects or buckets. Priority for implementation was then given to the ones deemed most commonly used and most relevant to most applications, however, this selection was entirely subjective. Table \ref{tab:cachedops} below shows all operations for which caching was implemented as part of \code{CacheLayer}. A full list of operations determined to be cachable can be found in (\ref{appendix:tab:cachedops}).
\begin{figure}[h]
	\begin{center}
		\begin{tabular}{| l |}
			\hline
			GetObject \\ \hline
			HeadBucket \\ \hline
			HeadObject \\ \hline
			ListBuckets \\ \hline
			ListObjects \\ \hline
			ListObjectsV2 \\ \hline
			ListObjectVersions \\ \hline
		\end{tabular}
	\end{center}
	\caption{List of cacheable and implemented operations}
	\label{tab:cachedops}
\end{figure}

\subsection{Cache policy and configuration}

The cache uses a hashmap as its underlying data structure and employs a TinyLFU (\ref{tiny_lfu}) admission policy together with an LRU eviction policy provided by the \code{moka}\cite{MOKA_GITHUB} library as cache policy. Entries in the cache are weighted by the size of the response body in bytes. For the cache, a global time-to-live (TTL), as well as a time-to-idle (TTI), can be configured. Additionally, caching for each supported S3 operation can be configured separately with its own TTL and TTI or can be disabled individually.

\begin{codeblock}{Example cache configuration}{TOML}
	\begin{javacode}
		[[middlewares]]
		cacheSize = 50000000
		ttl = 1000000
		tti = 1000000
		
		[middlewares.ops.GetObject]
		enabled = true
		
		[middlewares.ops.HeadObject]
		enabled = true
		
		[middlewares.ops.ListObjects]
		enabled = true
		
		[middlewares.ops.ListObjectVersions]
		enabled = true
		
		[middlewares.ops.HeadBucket]
		enabled = true
		
		[middlewares.ops.ListBuckets]
		enabled = true
	\end{javacode}
\end{codeblock}

\subsection{Cache headers and key generation}
\label{impl:httpheaders}
In a traditional HTTP cache, cache keys are commonly provided by the upstream server via the ETag response header. As described in (\ref{http_cache_policy}), the ETag header value uniquely identifies an underlying resource, and can be used together with the "If-None-Match" header to enable simple and reliable caching. Although widely used S3 systems like AWS S3 and MinIO support these headers, their effectiveness is often limited by the way S3 systems are accessed and interacted with.
In particular, software accessing S3 systems is often not designed to handle recurring requests intelligently. As such, they often do not make use of caching and HTTP caching mechanics and do not supply caches or servers with the required information like the "If-None-Match" headers. Another issue with caching S3 operations arises from the fact that many operations are not directly intended to be cached, although doing so is theoretically possible. In those cases, the S3 system will return responses without any caching-related headers.

To enable the caching logic to efficiently retrieve cached responses without this information, a new method to derive keys directly from requests instead of the response was required. Since the fields sent to the S3 API vary between different operations and not every field affects the cachability of a request, the key generation had to be implemented for each supported operation individually following these steps:
\begin{enumerate}
	\item The configuration (\ref{configuration}) is checked to determine if the operation is enabled.
	\item The request is parsed and deserialized and associated with the operation.
	\item The resulting type is checked for the existence of any fields causing the response to change unexpectedly, as well as some rarely used fields that excluded to simplify the logic. If any such a field is detected, the key generation fails.
	\item The key is generated from a combination of the operation's name and the fields that affect the response.
\end{enumerate}

\subsection{Update events}
\label{cache:event}
As part of the \textit{external composition} goal of this work, this cache also is capable of receiving update notifications for cached items from other upstream caches or upstream servers by listening to broadcast events provided by the webhook component. By configuring the upstream S3 server to send update events via the webhook, cache items can be efficiently evicted upon notice. This allows chaining multiple instances of this proxy without significantly increasing the risk of serving stale data due to additive TTL.
This feature could be further enhanced by directly refreshing entries instead of evicting them upon notice.

\section{S3 Client}

The client is the component responsible for resolving requests into responses, commonly by forwarding the request to an upstream server, although this is not strictly necessary. For example, a client could also be implemented to resolve requests by retrieving files directly from storage.

\begin{codeblock}{Client}{Rust}
	\begin{rustcode}
		pub trait Client: Send + Sync {
			/// Asynchrounously resolves a [Request] into a [Response]
			fn send(&self, request: Request) -> impl Future<Output = Result<Response, SendError>> + Send;
		}
	\end{rustcode}
\end{codeblock}

The \code{S3Client} is the S3-specific implementation of the \code{Client} trait, and its implementation is relatively simple. Its most striking feature is its relatively wide range of configuration options, including its ability to be configured to use HTTP/2 to send requests, which is usually not supported by S3 servers.
This can be used to provide more efficient communications between two instances of s3p.

\begin{codeblock}{Example client configuration}{TOML}
	\begin{javacode}
		[client]
		type = "s3"
		endpointUrl = "http://localhost:9000"
		forcePathStyle = true
		enableHttp2 = true
		insecure = true
		connectTimeout = 10_000
		readTimeout = 10_000
		operationTimeout = 10_000
		operationAttemptTimeout = 10_000
		maxRetryAttempts = 3
	\end{javacode}
\end{codeblock}

\section{Pipeline}

The Pipeline is a management structure composed of a ServerBuilder, any number of Middlewares and a Client. It is responsible for assembling the different components passed to it into a cohesive unit. When initialized with all required components it first constructs the RequestProcessor from the client and middlewares together with a broadcast channel that is registered with all components. The broadcast channel is used to pass events other than requests, like the ones produced by the Webhook component. It then converts the RequestProcessor into a Handler, which is then used to finalize the ServerBuilder to start the Server.

\section{Webhook}

The webhook is the component that enables receiving messages from an upstream server. Opon receiving an event message it parses and releases the message onto an event bus that all other components can connect to to implement additional behavior as was done in (\ref{cache:event}).

It is implemented by the \code{S3WebhookServer} to provide handling of event notifications as defined by AWS \cite{AWS_S3_USER_GUIDE}. This was done in regard to the \textit{external composition} goal of this work. Since event notifications are a non-standard feature for S3 servers, the handling of events required manual implementation for the parsing of event messages. An example of a message is included in (\ref{appendix:webhook}).

\section{Challenges}

During the implementation, some challenges were encountered with some of the libraries used by this implementation. These challenges are documented in this section.

\subsection{aws-sdk-s3}
The initial plan for implementation was to use the library aws-sdk-s3 which is part of the AWS SDK for Rust, a comprehensive set of libraries provided by AWS that provide integrations for many AWS services\cite{AWS_SDK_GITHUB}. It is widely used and commonly accepted as the default library to interact with S3 services and actively developed and maintained by AWS, making it a good choice for maintainability. For every operation it provides an individual request and response type to enable type-safe usage and predictable behavior.

This however proved challenging, as the features exposed by the library were found to be severely limited and even if the necessary features existed in theory, they were often not exposed to its users.
Most significantly, the implementation of the serialization and deserialization of its data types was encapsulated inside the client implementation and therefore not callable by users of the library.
Other issues were that there was no trait to implement unified handling for all operations. Instead, for each operation a separate implementation of every feature would have been necessary and that most fields on both the request and response types were private hidden with no setter methods provided. Request objects could also not be directly created by the user, but alwas needed to hold a reference to a client provided by the SDK, making its application callenging for this implementation.

An attempt to modifying the source code of aws-sdk-s3 was made, but that too proved challenging due to the entire SDK being generated through a code generator\cite{AWS_SDK_GITHUB} and its deep integration with other parts of the SDK.

\subsection{s3s}
s3s\cite{S3S_GITHUB} is an alternative implementation to the aws-sdk-s3 library and was used as a replacement to aws-sdk-s3. Similar to the AWS SDK, it is created from a combination of handwritten code and a code generator. This library proved less challenging than the implementation with aws-sdk-s3, but still some problems were encountered, and its code had to be modified to enable this implementation. This section documents the changes made to this library as part of this work.

\subsubsection{Unclonable types}
\label{s3s:meta}
Like aws-sdk-s3, s3s also provides individual request and response types for each operation, however most of these types did not implement Rust's \code{Clone} trait, so no copies could be made for instances of these types, but it was a necessity for the cache implementation to be able to store responses.
As part of this implementation, its code generator was modified to implement the missing trait where possible. For types containing fields that were not clonable by design (eg. byte streams), an additional clonable "Meta" type was generated without the field and the ability to convert split the initial type into its "Meta" type and the unclonable field.

\subsubsection{Selecting Operations}
Unlike aws-sdk-s3, s3s provided a trait to enable implementation of shared behaviors for all operations. However, the user only had access to a type-erased variant, which did not allow for determining the current operation performed by a request at runtime.

This was modified to provide a typed variant instead so that the user could implement different behaviors for individual operations where necessary. This was also required by the cache to implement custom handling based on the operation performed.

\subsubsection{Error handling}
Errors received from the upstream S3 server were hidden from the user, making it challenging to debug issues. This also showed to be an issue for responses with the return code "304 Not Modified" (see \ref{http_cache_policy}), as the library did not expect this code and returned an generalized error. For the implementation of \ref{http_cache_policy} this behavior was modified to enable further inspection of the response by the user.
